[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a recent graduate of Macalester College where I studied statistics with minors in geography and creative writing. Right now, I work at Metro Transit as a Research and Analytics data science intern. At this position, I got interested in transit demand modeling and completed a statistics honors thesis on fitting hierarchical spatial Bayesian models to Twin Cities bus ridership. I\u0026rsquo;m interested in continuing work like this and in topics like spatial statistics, geographic data science, and reproducible research.\nOutside of work, I love applying data science skills to new projects, like the National Gallery of Art\u0026rsquo;s first datathon. My team explored gender diversity in the Gallery\u0026rsquo;s collection and presented to Gallery curators and staff! When I\u0026rsquo;m not thinking about statistics, I love to read and write, knit, and rock climb.\n","date":1609027200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1609027200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ravenmcknight.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a recent graduate of Macalester College where I studied statistics with minors in geography and creative writing. Right now, I work at Metro Transit as a Research and Analytics data science intern. At this position, I got interested in transit demand modeling and completed a statistics honors thesis on fitting hierarchical spatial Bayesian models to Twin Cities bus ridership. I\u0026rsquo;m interested in continuing work like this and in topics like spatial statistics, geographic data science, and reproducible research.","tags":null,"title":"Raven McKnight","type":"authors"},{"authors":["Raven McKnight"],"categories":[],"content":"Introduction One of the last assignments I did before graduation was a tutorial on Kernel Density Estimation (KDE) for my classmates taking Mathematical Statistics. This is the tutorial and activities I wrote with my classmate, Kaden Bieger, to walk our class through the basics of KDE. Working through the activities in this tutorial should give you a good working understanding of Kernel Density Estimation!\nNonparametric statistics is a rapidly developing field. It is also very different than the parametric content covered in most Introduction to Mathematical Statistics classes! Broadly speaking, nonparametric methods allow us to relax assumptions about our data. We often assume our data comes from a normal distribution, or at the very least from a distribution with mean $\\mu$ and variance $\\sigma^2$. Nonparametric methods are not based on such parameters.\nKernel density estimation (KDE) is a common technique used to estimate probability density functions (PDFs). In practice, we rarely know much at all about the true distribution of our sampled data. KDE allows us to get an estimated PDF without making unreasonable assumptions of our data.\nIntuition KDE is a method you\u0026rsquo;ve seen before even if you don\u0026rsquo;t know it! The intuition behind KDE is similar to the intuition behind a simple histogram. Histograms can be used to visually approximate the distribution of data. There are a few reasons we want a more sophisticated method, however. First, as the plot below illustrates, histograms are very sensitive to the number and width of bins. Additionally, a histogram provides an excessively local estimate \u0026ndash; there is no \u0026ldquo;smoothing\u0026rdquo; between neighboring bins.\nIf you\u0026rsquo;ve ever made a density plot, you have even more experience with KDE. Behind the scenes, the ggplot2 function geom_density employs KDE! It turns out we can set the specific kernel and bandwidth when we call geom_density. We\u0026rsquo;ll cover kernels and bandwidths below.\nKernel Density Estimation The goal of kernel density estimation to to estimate the PDF of our data without knowing its distribution. We use a kernel as a weighting function to smooth our data in order to get this estimate.\nA kernel is a probability density function with several additional conditions:\n Kernels are non-negative and real-values Kernels are symmetric about 0  Several familiar PDFs, including the Gaussian and Uniform PDFs, meet these requirements. Once we have a kernel selected, we can implement KDE.\nGiven data $x = (x_1, x_2, \\ldots, x_n)$, a kernel function $K$, and a selected bandwidth $h$, the kernel density estimate at point $s$ is defined\n$$ \\hat{f}_{n(s)} = \\frac{1}{nh} \\sum_{i = 1}^n K (\\frac{x_i - s}{h}) $$\nThis is the kernel density estimator at a single point. To estimate an entire PDF, we apply the kernel to each point in our sample. The procedure is as follows:\nFirst, we apply the kernel function each data point in our sample. In the figure below, the blue points are our sample and the black lines are the kernel at each point.\nNext, we sum all $n$ kernel functions.\nFinally, we divide by $n$. Because each kernel function integrates to one, the resulting KDE will still integrate to one.\nIn notation, we can write this procedure as $f(x) = \\frac{1}{n} \\sum_{i=1}^{n} K(x - x_i)$ where $x-x_i$ centers the kernel function on each $x_i$. Note the similarity to the formal definition of a kernel density estimator above. The only additional parameter is the bandwidth!\nAcitivity 1 Consider 100 independent and identically distributed samples from an unknown distribution (plotted below). Given a bandwidth $h = 0.5$, write the Gaussian kernel density estimator for this sample at $s = 5$. We can use a standard normal with $\\sigma^2 = 1$ and $\\mu = 0$.\nTry plotting your calculated kernel density estimate using ggplot. You can compare your result to the output of geom_density\n# simulate some data to use set.seed(455) asimdat \u0026lt;- rchisq(n = 100, df = 5) asimdat \u0026lt;- data.frame(x = asimdat) my_est \u0026lt;- function(x, xi = 5, h = 0.5){ # your estimate 1/((100 * h) *sqrt(2*pi)) * exp(-1/2 * ((x - xi)/h)^2) } # to get the whole kde estimate, sum my_est over each s kern \u0026lt;- matrix(ncol = 100, nrow = 100) kde \u0026lt;- function(x, x_i = 5, h = 0.5){ for(i in 1:100){ kern[i, ] \u0026lt;- my_est(x = asimdat$x, x_i = asimdat$x[i], h = h) } colSums(kern) } ggplot(asimdat, aes(x=x)) + theme_minimal() + geom_density() + # generic geom_density geom_density(bw = 0.5, kernel = \u0026quot;gaussian\u0026quot;, color = \u0026quot;blue\u0026quot;) + #closer to your estimator stat_function(fun = my_est, color = \u0026quot;red\u0026quot;) + # your estimate at s = 5 geom_line(aes(x = x, y = kde(x)), color = \u0026quot;pink\u0026quot;) # and your overall estimate! Choosing a Kernel Function Kernel functions are non-negative, symmetric, and decreasing for all $x \u0026gt; 0$ (because they\u0026rsquo;re symmetric, this also means they are increasing for all $x \u0026lt; 0$). Gaussian and Rectangular (uniform) are two kernel choices, though there are many others. The plot below shows kernel density estimates for the same simulated data using four common kernels. In this example, we use a fairly low bandwidth (0.3), to make differences between the kernels clear.\nChoice of kernel doesn\u0026rsquo;t affect end result that much, although there are benefits to some kernels in particular. The Epanechnikov kernel, for example, is MSE optimal.\nBonus Activity: If you want to try out other kernel options inside geom_density, go to \u0026ldquo;Help\u0026rdquo; in Rstudio and search for \u0026ldquo;density.\u0026rdquo; Scroll down to \u0026ldquo;arguments\u0026rdquo; to see all possible kernels!\nChoosing Bandwidth The bandwidth is the width of our smoothing window \u0026ndash; following the histogram example, this is like the width of bins. Bandwidth is generally represented as $h$. Bandwidth is how we control the smoothness of our estimate. The figure below shows a Gaussian KDE with various bandwidths.\nBandwidth tradeoffs A small $h$ places more weight on the data, while a large $h$ performs more smoothing. We can use smaller bandwidths when $n$ is larger, particularly when the data has a small range (ie it is tightly packed). Larger $h$ is better when we have less data or more spread out observations.\nThere\u0026rsquo;s an analogy here with Bayesian priors \u0026ndash; choosing $h$ lets us choose how much weight we want to give the data.\nActivity 2 Using the kernel you defined above, plot a KDE estimate using various values for $h$. You can do so by updating the bandwidth you defined in my_est. For comparison, you can also use the parameter bw within a geom_density. How might you choose an optimal bandwidth?\nWe can get KDE estimates using the KDE function we wrote above.\nggplot(asimdat, aes(x = x, y = kde(2))) + geom_line() + theme_minimal() Or, more simply, we can use the built in geom_density function.\nggplot(asimdat, aes(x = x)) + geom_density(bw = 2) + theme_minimal() Bias, Variance, MSE Bias Naturally, we would like to consider the bias and mean squared error of estimators produced by kernel density estimation. In this section, we outline proofs deriving the expected value, bias, and mean squared error for kernel density estimates. The proofs here are somewhat simplified but largely rely on u-substitution and Taylor expansions.\nGiven observations $x_1, x_2, \\ldots, x_n \\overset{iid}{\\sim} f$, we define the expected value of our estimator as follows:\n$$ \\begin{aligned} \\mathbf{E}[\\hat{f}_n(s)] \u0026amp;= \\mathbf{E}\\left[\\frac{1}{nh} \\displaystyle \\sum_{i=1}^n K \\left( \\frac{x_i - s}{h}\\right) \\right] \\\\ \u0026amp;= \\frac{1}{h}\\mathbf{E}\\left[K \\left( \\frac{x - s}{h}\\right) \\right] \\\\ \u0026amp;= \\frac{1}{h} \\int K\\left( \\frac{x - s}{h}\\right) f(x) dx \\end{aligned} $$\nThis first line follows simply from the definitions of kernel density estimators and expected value.\nNext, we let $u = \\frac{x - s}{h}$ and substitute:\n$$ \\mathbf{E}[\\hat{f}_n(s)] = \\frac{1}{h} \\int K\\left( u\\right) f(hu + s) du $$\nThen, we apply the 2nd order Taylor expansion for $f(hu + s)$ about $h = 0$. Omitting several steps of algebra, our Taylor expansion can be written\n$$ \\begin{aligned} f(hu + s) \u0026amp;= f(s) + \\frac{f'(s)}{1!}(u)(h-0) + \\frac{f''(s)}{2!}(u^2)(h-0)^2 + o(h^2) \\\\ \u0026amp;= f(s) + huf'(s) + \\frac{h^2u^2}{2}f''(s) + o(h^2) \\end{aligned} $$\nwhere $o(h^2)$ is some function which approaches zero as $h$ approaches infinity. Technically, this $o()$ notation indications that $o(h^2)$ becomes negligible compared to $h^2$ as $h \\rightarrow 0$. For our purposes, we assume $o(h^2) \\rightarrow 0$ as $h \\rightarrow 0$.\nWe plug the Taylor expansion into our expected value above and simplify via algebra.\n$$ \\begin{aligned} \\mathbf{E}[\\hat{f}_n(s)] \u0026amp; = \\int K(u) \\left[f(s) + huf'(s) + \\frac{h^2u^2}{2}f''(s) + o(h^2)\\right] du \\\\ \u0026amp; = f(s)\\int K(u) du + hf'(s)\\int uK(u) du + \\frac{h^2}{2}f''(s)\\int u^2 K(u)du + o(h^2) \\\\ \u0026amp; = f(s) + \\frac{h^2}{2}f''(s)\\int u^2 K(u)du + o(h^2) \\end{aligned} $$\nWe can plug this into the definition of bias such that\n$$ \\begin{aligned} \\textbf{Bias}(\\hat{f}_n(s)) \u0026amp;= E[\\hat{f}_n(s)] - f(s) \\\\ \u0026amp;= \\frac{h^2}{2}f''(s)\\int u^2 K(u)du + o(h^2) \\\\ \u0026amp;= \\frac{t \\cdot h^2}{2}f''(s) + o(h^2) \\end{aligned} $$\nwhere $t = \\int u^2 K(u)du$.\nActivity 3 What happens to bias as bandwidth $h$ increases/decreases?\nAs $h$ increases, bias increases (and vice versa).\nVariance The proof for variance is very similar to the proof for bias. This proof is possible because $K$ is symmetric about 0. Feel free to work through this proof more on your own!\n$$ \\begin{aligned} \\mathbf{Var}(\\hat{f}_n(s)) \u0026amp;= \\mathbf{Var} \\left(\\frac{1}{nh} \\displaystyle \\sum_{i=1}^n K \\left(\\frac{x_i - s}{h}\\right)\\right) \\\\ \u0026amp;= \\frac{1}{nh^2} \\left(\\mathbf{E}\\left[ K^2 \\left( \\frac{x - s}{h}\\right) \\right] - \\mathbf{E}\\left[ K \\left( \\frac{x - s}{h}\\right)\\right]^2\\right)\\\\\\\\\\\\ \u0026amp;\\leq \\frac{1}{nh^2} \\mathbf{E}\\left[ K^2 \\left( \\frac{x - s}{h}\\right) \\right] \\\\ \u0026amp;= \\frac{1}{nh^2} \\int K^2 \\left( \\frac{x - s}{h}\\right)f(x) dx \\end{aligned} $$\nAs above, we substitute $u = \\frac{x-s}{h}$ and plug in a 1st order Taylor expansion of $f(hu + s)$.\n$$ \\begin{aligned} \\mathbf{Var}(\\hat{f}_n(s)) \u0026amp;\\leq \\frac{1}{nh^2} \\int K^2(u)f(hu + s)hdu \\\\ \u0026amp;= \\frac{1}{nh} \\int K^2(u)f(hu + s)du \\\\ \u0026amp;= \\frac{1}{nh} \\int K^2(u)[f(s) + huf'(s) + o(h)]du \\\\ \u0026amp;= \\frac{1}{nh} \\bigg(f(s)\\int K^2(u) du + hf'(s)\\int uK^2(u) du + o(h)\\bigg) \\\\ \\mathbf{Var}(\\hat{f}_n(s)) \u0026amp;\\leq \\frac{f(s)}{nh}\\int K^2(u) du + o\\bigg(\\frac{1}{nh}\\bigg) \\\\ \u0026amp;= \\frac{z}{nh}f(s) + o\\bigg(\\frac{1}{nh}\\bigg) \\end{aligned} $$\nwhere $z = \\int K^2(u) du$.\nActivity 4 What happens to variance as $h$ changes? As $n$ changes?\nVariance decreases as both $h$ and $n$ increase (and vice versa).\nActivity 5 Given the Bias and Variance above, find the Mean Squared Error of our estimator. Recall that the formula for MSE is $Var() + Bias^2()$.\nAsymptotic MSE Given the MSE we derived above, we can see that the Asymptotic MSE (AMSE) is $\\frac{t^2h^4}{4}\\left[f''(s)\\right]^2 + \\frac{z}{nh}f(s)$. Often, we use the AMSE to optimize $h$ at a given point.\nActivity 6 Try optimizing the AMSE in terms of $h$. If you\u0026rsquo;re up for a challenge, try finding the optimal $h$ for the sample in Activity 1 (with $h = 0.5$ and $s = 5$)!\n$$ \\begin{aligned} \\frac{\\partial}{\\partial h} \\textbf{AMSE}(\\hat{f}_n(s)) \u0026amp;= \\frac{\\partial}{\\partial h}\\left(\\frac{t^2}{4}\\left[f''(s)\\right]^2\\right)\\mathbf{h^4} + \\left(\\frac{z}{n}f(s)\\right)\\mathbf{\\frac{1}{h}} \\\\ \u0026amp;= \\left(t^2\\left[f''(s)\\right]^2\\right)\\mathbf{h^3} - \\left(\\frac{z}{n}f(s)\\right)\\mathbf{\\frac{1}{h^2}} \\end{aligned} $$\nWe can rewrite this as follows to solve for the optimal value of $h$, $h_{opt}$\n$$ \\begin{aligned} 0 \u0026amp;= \\left(t^2\\left[f''(s)\\right]^2\\right)\\mathbf{h^5} - \\left(\\frac{z}{n}f(s)\\right) \\\\ \\mathbf{h_{opt}} \u0026amp;= \\left(\\frac{zf(s)}{nt^2\\left[f''(s)\\right]^2}\\right)^{\\frac{1}{5}} \\end{aligned} $$\nAn Open Question: AMISE The optimization in Activity 6 optimizes $h$ at a given $x$ value. A common method for optimizing $h$ across an entire distribution is using the asymptotic mean integrated square error (AMISE). As the name suggests, we can define the AMISE as follows:\n$$ \\textbf{AMISE}(\\hat{f}_n(s)) = \\int \\left(\\frac{t^2h^4}{4}\\left[f''(s)\\right]^2 + \\frac{z}{nh}f(s) \\right) dx $$\nIt\u0026rsquo;s possible to optimize AMISE in terms of $h$ to get the optimal bandwidth across an entire sample. We won\u0026rsquo;t ask you to do this \u0026ndash; it\u0026rsquo;s definitely a challenge problem! In the end, you\u0026rsquo;d find that the optimal $h$ is dependent upon $\\int \\left[f''(x)\\right]^2 dx$, or the curvature of the underlying PDF. Many recent advancements in KDE studies have been in the realm of estimating AMISE or the underlying curvature in order to optimize $h$.\n","date":1609027200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609027200,"objectID":"3ee809cf5e52fdd2bf824e3ac702c8f6","permalink":"https://ravenmcknight.com/post/introduction-to-kde/","publishdate":"2020-12-27T00:00:00Z","relpermalink":"/post/introduction-to-kde/","section":"post","summary":"A tutorial on the basics of nonparametric probability density function estimation.","tags":null,"title":"Introduction to Kernel Density Estimation","type":"post"},{"authors":[],"categories":[],"content":"In the spring of 2020, I got to take a class called Cultural Atlas Production at Macalester. Inspired by cartographers like Rebecca Solnit, and Macalester\u0026rsquo;s 2019 cultural atlas of Saint Paul, the class created a cultural atlas of Minneapolis!\nMy spread focused on The Amazon Bookstore Cooperative, the oldest lesbian bookstore in the country. It existed in Minneapolis from the early 1970s until it closed in 2012.\nLike so many things, this project was affected by COVID-19 when lockdown began. Despite those challenges, I\u0026rsquo;m so impressed with everything my classmates created. You can see the whole cultural atlas and read more about the project here!\n","date":1607040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607040000,"objectID":"28d10edeb0d77035c1df57001b78e068","permalink":"https://ravenmcknight.com/project/cultural-atlas/","publishdate":"2020-12-04T00:00:00Z","relpermalink":"/project/cultural-atlas/","section":"project","summary":"A Cultural Atlas of Minneapolis!","tags":[],"title":"Meandering Minneapolis","type":"project"},{"authors":[],"categories":[],"content":"I was recently inspired by Alyssa Fowers visualization of her fall semester coffee consumption. I tracked my own coffee-drinking data for a month to create this plot!\n\nIt was fun to see where I drank abnormal amounts of coffee, or coffee at abnormal times \u0026ndash; like the office hours I hold on Mondays at 7pm or during my trip to DC towards the end of the month. I was also surprised by how much the time of my first coffee varied \u0026ndash; I definitely expected to see a stronger \u0026ldquo;6am coffee\u0026rdquo; pattern.\nI created this plot with ggplot2, Illustrator, and a Gilmore Girls reference for good measure. What would a coffee viz be without Lorelai, after all?\nCode for the plot is on my Github. Here\u0026rsquo;s to sleeping more in November!\n","date":1573010341,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573010341,"objectID":"96d6a55787610f1ad602503ddd59fe02","permalink":"https://ravenmcknight.com/project/october-in-coffee/","publishdate":"2019-11-05T21:19:01-06:00","relpermalink":"/project/october-in-coffee/","section":"project","summary":"A visualization of all the coffee I drank in October, 2019.","tags":[],"title":"October in Coffee","type":"project"},{"authors":[],"categories":[],"content":"I designed this font from scratch in my 2D Design class by drawing, scanning, and digitizing each letter. It was a ton of fun, and a great opportunity to design a poster for one of my favorite musicians \u0026amp; songs!\n\n","date":1572724020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572724020,"objectID":"4772e822c8907ccf8f882a3eb97fa9e0","permalink":"https://ravenmcknight.com/project/alaska-typeface/","publishdate":"2019-11-02T14:47:00-05:00","relpermalink":"/project/alaska-typeface/","section":"project","summary":"A typeface and poster I designed, inspired by Maggie Rogers!","tags":[],"title":"Alaska Typeface","type":"project"},{"authors":[],"categories":[],"content":"This StoryMap was created in collaboration with my Urban GIS class at Macalester. The class consists of a semester-long project supporting a community partner \u0026ndash; in our case, Saint Paul City Councilmember Rebecca Noeker and the Saint Paul 3K Initiative.\n\nThe goal of Saint Paul 3K is to provide affordable, accessible childcare to all 3- and 4-year olds in Saint Paul. This is the second time Urban GIS has partnered with Councilmember Noeker. Our work focused on primary data collection and identifying barriers to care across Saint Paul. Check out the full StoryMap here!\n\n","date":1572724020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572724020,"objectID":"98b76e11d61ab33523ca9d069e72ba49","permalink":"https://ravenmcknight.com/project/saint-paul-3k/","publishdate":"2019-11-02T14:47:00-05:00","relpermalink":"/project/saint-paul-3k/","section":"project","summary":"A geography class project supporting Saint Paul City Council","tags":[],"title":"Saint Paul 3K","type":"project"},{"authors":[],"categories":[],"content":"I drew these patterns in Illustrator using a Wacom tablet. This was a really fun blend of doodling and digital art!\nI was inspired by dark, earthtone paisleys I saw on clothing growing up for my first pattern.\n\nI reused some of the same elements in my second pattern, and tried to incorporate each of the plants I have in my bedroom right now!\n\n","date":1572724020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572724020,"objectID":"7c00fe868a9904ce54c8c360e573273c","permalink":"https://ravenmcknight.com/project/tossed-patterns/","publishdate":"2019-11-02T14:47:00-05:00","relpermalink":"/project/tossed-patterns/","section":"project","summary":"Recently, I learned to create tossed patterns using Adobe Illustrator.","tags":[],"title":"Tossed Patterns","type":"project"},{"authors":[],"categories":[],"content":"Our final project in my 2D Design class was called \u0026ldquo;Teach Me\u0026rdquo; \u0026ndash; we were meant to design something educational. My mind went straight to census geography! I designed this guide to be printed as a zine, but you can view a pdf of the pages here.\n\nI covered five of the most common geographies, and focused on the county I live in right now. I also highlighted my favorite coffee shop to give a sense of scale. I even used the typeface I designed earlier in the semester! This was a super fun project to end the semester with :-)\n","date":1572724020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572724020,"objectID":"96a1f00237cd29a4489a2462956c4042","permalink":"https://ravenmcknight.com/project/census-geog-guide/","publishdate":"2019-11-02T14:47:00-05:00","relpermalink":"/project/census-geog-guide/","section":"project","summary":"My final project for 2D Design!","tags":[],"title":"Wannabe Geographer's Guide to Census Geography","type":"project"},{"authors":[],"categories":[],"content":"This is the app I created in collaboration with the National Gallery of Art. I would consider this app in the \u0026ldquo;prototype\u0026rdquo; stage \u0026ndash; my goal was to illustrate the types of questions that can be answered with the data the Gallery already collects. Check out the app here, or hear me talk more about it here.\n","date":1572614737,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572614737,"objectID":"a4abbf5adce4d50a9fe73b6ba620d2cc","permalink":"https://ravenmcknight.com/project/intent-in-acquisition/","publishdate":"2019-11-01T08:25:37-05:00","relpermalink":"/project/intent-in-acquisition/","section":"project","summary":"A robust Shiny app developed to explore the National Gallery of Art's collection data.","tags":[],"title":"Intent in Acquisition","type":"project"},{"authors":["Raven McKnight"],"categories":null,"content":"This is a tutorial on fitting spatial Bayesian models using the R package CARBayes. It was written in collaboration with Katie Jolly at Macalester! Here, we\u0026rsquo;ll help walk you through how to start an analysis of spatial data using the CARBayes package.\nIntroduction to spatial data Spatial data is everywhere. It can describe geography, demographics, epidemiology, and more. In fact, a lot of data we don’t always think of as spatial, such as voting records, can also be understood as describing variation across space. Unfortunately, spatial data breaks one of the fundamental assumptions of statistics\u0026ndash;that individual observations are independent from one another.\nWe know that spatial data isn’t independent intuitively: the price of housing or risk of contracting a disease in one county, for example, is likely related to the county next door. This understanding of spatial data allows us to borrow strength from neighbors in our analysis. If we’re missing data for a county, we can make some inference about the data we’re missing based on the counties around it.\nTo appropriately model spatial data, we need to take this spatial autocorrelation into account. Spatial autocorrelation is generally represented as a value between -1 and 1, where 1 represents clustering and -1 represents dispersion.\nIn this tutorial, we’ll use the hierarchical nature of spatial data to our advantage. Hierarchical data is data with some underlying structure. Common examples of hierarchical data are longitudinal data, were we have multiple observations on multiple subjects over time, or in our case, spatial data, where data inherits the spatial structure of geography.\nBayesian Methods The methods we’ll use in this tutorial are drawn from Bayesian Statistics, which allow us to incorporate prior understanding of our data into our models. The idea of Bayesian methods is to reach a balance between our prior understandings and incoming information in the form of data. For more on Bayes, check out this article!\nBayesian methods are particularly suited to modelling hierarchical data because we can account for the hierarchy using our priors. For example, a simple hierarchical model might say that the percent of a county who voted for Trump in the 2016 election is centered around a different mean value for each state (top layer of the model) and the mean value for each state is based on some other parameters, such as median income (priors).\nConditional autoregressive models In this tutorial, we’ll fit conditional autoregressive (CAR) models. These are Bayesian models used to describe data in non-overlapping areal units, such as counties or states. CAR models are often used to model things like disease rates or ecological phenomena, but they can also be used to model housing prices, for example.\nIf $Y_i$ is the response for area $i$ and $x_i$ is a set of covariates or characteristics in the $i$th area, the formal model statement is as follows:\n$$ \\begin{aligned} E(Y_i | Y_j, j\\not= i) \u0026amp;= x_i^T\\beta + \\sum^n_{j=1}c_{ij}(Y_j - x^T_j\\beta) \\\\\\\\\\\\ Var(Y_i | Y_j, j\\not= i) \u0026amp;= \\tau^2_i = \\tau^2/w_{i.} \\end{aligned} $$\nwhere $w_{i.} = \\sum^n_{j=1}w_{ij}$, $c_{ij}$ is nonzero only if $Y_j \\in N_i$, and $N_i$ is the neighborhood of $Y_i$. $c_{ij}$ is typically $\\lambda w_{ij}/w{i}$.\nIn this tutorial, we’ll fit one of the most simple (and flexible) CAR models. This model is a Gaussian Markov random field (GMRF) model proposed by Besag (we’ll refer to it as the Besag model). If you’re familiar with time series models, the most basic GMRF is the AR(1) process. In simple terms, a Gaussian random field is a random function over space with Gaussian probability distributions. A Gaussian Markov random field means that we are only interested in locations one “step” away from our current location \u0026ndash; in this context of spatial data, this means that we’re interested in neighbors which directly share a boundary.\nThe Besag model generally looks like a random effects model:\n$$ \\begin{aligned} log(\\hat Y) \u0026amp;=\\mu+z_i^T\\beta + b_i \\\\ \\mu \u0026amp;= \\text{global mean} \\\\ z_i\\beta_i \u0026amp;= \\text{vector of covariates \u0026amp; corresponding coefficient} \\\\ b_i \u0026amp;= \\text{spatial random effect} \\end{aligned} $$\nWhen we define a prior distribution the default is often as follows:\n$$ \\mu \\sim N(0, \\frac{1}{100^2}) \\\\ \\beta \\sim N(0, \\frac{1}{100^2}) \\\\ b_i \\sim \\text{some spatial dependency model} $$\nThe Besag model considers $b_i$ to be normally distributed with the mean being a function of the neighboring values, the set $\\delta i$ and the variance proportional to the number of neighbors, $n_{\\delta i}$. In notation, it looks like this:\n$$ \\begin{aligned} b_i|b_{-i}, \\tau_b \u0026amp;\\sim N(\\frac{1}{n_{\\delta i}}\\sum_{j \\in \\delta i}b_j, \\frac{1}{n_{\\delta i} \\tau_b}) \\\\ \\tau_b \u0026amp;= \\text{precision parameter}\\\\ b_{-i} \u0026amp;= (b_1, ... b_{i-1}, b_{i+1},...,b_n)^T \\end{aligned} $$\nThere are several natural progressions from the Besag model. For more accurate parameter estimates in the case of overdispersion (excessive variance) or unstructured errors (errors with no spatial autocorrelation), we can use the Besag-York-Mollíe model. Further, to account for spatial autocorrelation and overdispersion with two separate parameters, we can use the Leroux model. More information on each of these models can be found here.\nData and packages We\u0026rsquo;ll fit these models using the CARbayes package in R. This package allows us to fit several univariate and multivariate CAR models and make inference in a Bayesian context using Markov chain Monte Carlo (MCMC) simulations. The package supports response variable from Gaussian, multinomial, Poison (or count), and zero-inflated Poisson distributions. Spatial autocorrelation is modeled using random effects in most cases, and each function in the package corresponds to a different prior or model type. Reacall we\u0026rsquo;ll be using Besag priors, which assume a constant degree of spatial autocorrelation among the observations. The random effects are distributed normally around the mean of the neighbors and the variance is proportional to the size of the neighborhood. There is a lot of potential complexity built into the CARbayes package but for this tutorial, we’ll focus on simple application and interpretation!\nYou\u0026rsquo;ll need the packages below to run this tutorial on your own. This code chunk makes sure that all of the packages are installed and loaded!\npackages \u0026lt;- c(\u0026quot;CARBayes\u0026quot;, \u0026quot;sf\u0026quot;, \u0026quot;tidycensus\u0026quot;, \u0026quot;janitor\u0026quot;, \u0026quot;tidyverse\u0026quot;, \u0026quot;spData\u0026quot;, \u0026quot;spdep\u0026quot;) miss_pkgs \u0026lt;- packages[!packages %in% installed.packages()[,1]] if(length(miss_pkgs) \u0026gt; 0){ install.packages(miss_pkgs) } invisible(lapply(packages, library, character.only = TRUE)) rm(miss_pkgs, packages) We will use a shapefile of Columbus, OH neighborhoods from the spData package. In total there are 49 neighborhoods and 22 variables and the data is from 1980.\ncolumbus \u0026lt;- columbus_sf \u0026lt;- st_read(system.file(\u0026quot;shapes/columbus.shp\u0026quot;, package=\u0026quot;spData\u0026quot;)[1]) It includes a variety of characteristics about the homes and people in each neighborhood.\n   Variable Description     NEIG neighborhood id value (1-49)   HOVAL median housing value (in $1,000)   INC household income (in $1,000)   CRIME number of residential burglaries and vehicle thefts per thousand households   OPEN open space in neighborhood (mi^2)   PLUMB percentage housing units without plumbing   DISCBD distance to the central business district (miles)    We will model the relationship between crime CRIME and home value HOVAL. In order to see whether or not spatial models are a necessary complexity, we will do some exploratory data analysis. If we do not see clear spatial patterns, we can use Generalized Linear Models and do not need to account for the spatial autocorrelation.\nThe first step in our exploration is to plot maps of our variables of interest.\nmap_theme \u0026lt;- theme_minimal() + theme(text = element_text(color = \u0026quot;#60717a\u0026quot;), panel.grid = element_line(\u0026quot;transparent\u0026quot;), axis.text = element_blank()) map_colors \u0026lt;- scale_fill_gradientn(colors = c(\u0026quot;#FFBD71\u0026quot;, \u0026quot;#FCA464\u0026quot;, \u0026quot;#F87D7B\u0026quot;, \u0026quot;#D04A73\u0026quot;), na.value = \u0026quot;#e1e5e8\u0026quot;) ggplot(columbus) + geom_sf(aes(fill = HOVAL)) + map_theme + map_colors + labs(title = \u0026quot;Median home values in Columbus, OH neighborhoods, 1980\u0026quot;, fill = \u0026quot;Home value in 1,000s\u0026quot;) ggplot(columbus) + geom_sf(aes(fill = CRIME)) + map_theme + map_colors + labs(title = \u0026quot;Residential crime in Columbus, OH neighborhoods, 1980\u0026quot;, fill = \u0026quot;Residential burglaries and vehicle \\nthefts per thousand households\u0026quot;) In these maps we can see two different important characteristics. First, it looks like there is spatial clustering for each of the variables individually. We will quantify this with Moran\u0026rsquo;s I. Second, we see an inverse relationship between these two variables. In context, this means that high home values and low residential crime rates are often found in the same neighborhood.\nBefore fitting a model, we will test the degree of spatial correlation with Moran\u0026rsquo;s I. A priori we can see that the pattern is likely clustering rather than dispersion, so we\u0026rsquo;ll conduct a one-tailed hypothesis test to see if the clustering is greater than random.\n$$H_O: \\text{no spatial clustering} \\\nH_A: \\text{spatial clustering}$$\ncol_sp \u0026lt;- as(columbus, \u0026quot;Spatial\u0026quot;) col_nb \u0026lt;- poly2nb(col_sp) # queen neighborhood col_listw \u0026lt;- nb2listw(col_nb, style = \u0026quot;B\u0026quot;) # listw version of the neighborhood W \u0026lt;- nb2mat(col_nb, style = \u0026quot;B\u0026quot;) # binary structure moran.mc(col_sp$HOVAL, listw = col_listw, nsim = 999, alternative = \u0026quot;greater\u0026quot;) # moran's I test HOVAL # Monte-Carlo simulation of Moran I # # data: col_sp$HOVAL # weights: col_listw # number of simulations + 1: 1000 # # statistic = 0.22134, observed rank = 991, p-value = 0.009 # alternative hypothesis: greater For home value, the moran.mc functionr returns a test statistic of 0.22134 (moderate positive correlation) and based on our Monte Carlo simulations of possible random patterns we have a p-value of 0.008. We reject the null hypothesis because there is substantial evidence of spatial clustering. Now we will repeat this process for crime. If both of these show evidence of clustering then we know we should use spatial models.\nmoran.mc(col_sp$CRIME, listw = col_listw, nsim = 999, alternative = \u0026quot;greater\u0026quot;) # moran's I test CRIME # Monte-Carlo simulation of Moran I # # data: col_sp$CRIME # weights: col_listw # number of simulations + 1: 1000 # # statistic = 0.51546, observed rank = 1000, p-value = 0.001 # alternative hypothesis: greater For crime, the test statistic is 0.51546 (strong clustering) and based on our Monte Carlo simulations we have a p-value of 0.001. We reject the null hypothesis because there is substantial evidence of spatial relationships.\nSpatial models In order to better quantify the relationship between these two variables accounting for their spatial structure we will use the models proposed by Besag and implemented in CARbayes. In order to run these models, we will use the weights matrix W defined above and model HOVAL by CRIME from the columbus data. In this model we assume a fixed spatial dependence parameter $\\rho = 1$ which simplifies the Leroux model to a Besag model (both can be fit using the same function). This model takes around 30 seconds to fit.\nmodel.hoval \u0026lt;- CARBayes::S.CARleroux(HOVAL ~ CRIME, data = columbus, W = W, family = \u0026quot;gaussian\u0026quot;, burnin = 20000, n.sample = 100000, thin = 10) model.hoval # ################# # #### Model fitted # ################# # Likelihood model - Gaussian (identity link function) # Random effects model - Leroux CAR # Regression equation - HOVAL ~ CRIME # Number of missing observations - 0 # # ############ # #### Results # ############ # Posterior quantities and DIC # # Median 2.5% 97.5% n.effective Geweke.diag # (Intercept) 60.6232 50.5268 70.5352 7684.7 -0.6 # CRIME -0.6338 -0.8887 -0.3754 8000.0 0.6 # nu2 226.4746 157.6672 346.1224 7527.6 1.1 # tau2 0.0084 0.0021 0.0962 1261.3 -0.9 # rho 0.3730 0.0170 0.9148 3293.0 1.0 # # DIC = 410.0037 p.d = 2.895076 LMPL = -205.92 In a Bayesian analysis we are simulating values from the posterior distribution of our parameters. If the middle 95% of distribution (the credible interval) does not cross zero then we can say it is a significant predictor. In this case we are looking at the CRIME parameter and we can see that the credible interval is below 0, meaning there is a significant negative or inverse relationship between crime and home value. The median value of the posterior distribution for CRIME is -0.715, meaning that for every crime recorded in that neighborhood the home value (in thousands) decreases by 0.715. Contextually, this makes sense. If we wanted to compare this model to others, we would use the DIC value, which is 97.4. We would select the model with the lower DIC, but this method is only applicable when the posterior distribution is approximately multivariate normal.\nWe can extract all of the simulated $\\beta_\\text{crime}$ coefficients and plot them to see the distribution.\ncbus_sim \u0026lt;- data_frame(beta_crime = as.vector(model.hoval$samples$beta[,2])) ggplot(cbus_sim, aes(x = beta_crime)) + geom_density(fill = \u0026quot;#FFBD71\u0026quot;, color =\u0026quot;#FFBD71\u0026quot;) + geom_vline(xintercept = -0.8936, color = \u0026quot;#F87D7B\u0026quot;, size = 1.5) + geom_vline(xintercept = -0.3690, color = \u0026quot;#F87D7B\u0026quot;, size = 1.5) + annotate(\u0026quot;text\u0026quot;, x = -0.65, y = 3.5, label = \u0026quot;Credible interval\u0026quot;, color = \u0026quot;#F87D7B\u0026quot;) + theme_minimal() + theme(text = element_text(color = \u0026quot;#60717a\u0026quot;)) + labs(title = \u0026quot;Posterior distribution of the crime coefficient\u0026quot;, x = \u0026quot;Beta\u0026quot;, y = \u0026quot;Density\u0026quot;) This is a good visual representation of the credible interval; we can see that our coefficient will almost never be 0!\nEvaluating the spatial pattern of residuals We also want to test to make sure our residuals aren\u0026rsquo;t clustered. If they are, that means we are missing an important confounding variable. We can check for this clustering with the same Moran\u0026rsquo;s I test on our residual.\nmoran.mc(x = as.vector(model.hoval$residuals$response), listw = col_listw, nsim = 9999, alternative = \u0026quot;greater\u0026quot;) # Monte-Carlo simulation of Moran I # # data: as.vector(model.hoval$residuals$response) # weights: col_listw # number of simulations + 1: 10000 # # statistic = 0.17135, observed rank = 9819, p-value = 0.0181 # alternative hypothesis: greater In this example we do see slight clustering in our residuals (I = 0.17), but that is likely because we only used one covariate to describe home patterns. In later iterations we would include more covariates in the model. We can also map the residuals to see where these clusters appear.\ncolumbus %\u0026gt;% mutate(resid = model.hoval$residuals$response) %\u0026gt;% ggplot(aes(fill = resid)) + geom_sf() + map_theme + map_colors + labs(title = \u0026quot;Residuals from the CAR model\u0026quot;, subtitle = \u0026quot;home value ~ crime\u0026quot;, fill = \u0026quot;Residual\u0026quot;) In order to choose our next covariate, we can start by thinking about what kinds of variables might explain this pattern in the residual. One idea might be to include distance to the Central Business District!\nDiscussion The model we’ve fit in this tutorial are a great place to start with Bayesian CAR models. The Besag model gives us fairly familiar results: coefficients which we can interpret just as we might in a more basic linear regression context. However, it gives more accurate predictions because it accounts for the underlying structure in the data. There are many layers of complexity we can add to make these models do more for us. Many of those improvements simply require adding new parameters or constraints to the Besag model!\nOne limitation of these CAR models is that they are parametric. This could be a strength or a limitation depending on the data we’re interested in! The methods are also computationally expensive \u0026ndash; when working with large amounts of data or covariates, a single model can take several minutes to run. This is less a weakness of the methodology and more a symptom of working with complex spatial data.\nAs a package, CARBayes is incredibly versatile. There are several other models and corresponding functions in the package which we haven’t discussed, so if the models presented here don’t fit your needs, check out the vignette! If you\u0026rsquo;ve got any other questions, feel free to reach out!\n","date":1556755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556755200,"objectID":"99a8fd5be9351cddb8cb3988c0706882","permalink":"https://ravenmcknight.com/post/carbayes-tutorial/","publishdate":"2019-05-02T00:00:00Z","relpermalink":"/post/carbayes-tutorial/","section":"post","summary":"A tutorial for fitting conditional autoregressive models in R using the CARBayes package.","tags":null,"title":"CARBayes Tutorial","type":"post"}]